{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4주차3강_딥러닝_원리[2].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbR1GYVj8KIoSve2FHNAJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jang262/jang262.github.io/blob/master/4%EC%A3%BC%EC%B0%A83%EA%B0%95_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9B%90%EB%A6%AC%5B2%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkL1RBBEWaMs",
        "colab_type": "text"
      },
      "source": [
        "실제 신경망의 손실 함수 식은 다차 함수이므로  \n",
        "정의하거나 미분하기 어려울 때가 많습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84732409-9b349c80-afd6-11ea-86d9-58f6260b5097.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]1\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "신경망의 목적은 손실 함수가 최소값일 때의  \n",
        "파라미터를 찾아 올바른 학습 결과를 내는 것이죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84732775-8c9ab500-afd7-11ea-90cc-9aecdf1f7c70.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]2\" alt=\"RubberDuck\"></img><br/>\n",
        "이는 회귀분석이나 로지스틱 회귀와 기본개념이 같다는 걸 알 수 있어요.  \n",
        "단, 회귀분석에서 사용하는 파라미터의 개수보다  \n",
        "신경망에서 사용하는 파라미터의 개수가 더 많은 편이죠.  \n",
        "그래서 과거 수많은 학자들은 이러한 가중치를 효율적으로 찾고자 다양한연구를 진행했으며,  \n",
        "\n",
        "1987년 이 서적을 통해 역전파가 세상에 공개되었죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84732885-c53a8e80-afd7-11ea-87b0-0091d079e33a.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]3\" alt=\"RubberDuck\"></img><br/>\n",
        "이러한 연구 성과로 1960년대부터 연구했던 신경망은  \n",
        "다시 부활의 신호탄을 쏘아 올리게 됩니다.  \n",
        "\n",
        "신경망의 역전파는 그 이름에서도 알 수 있듯,  \n",
        "뉴런의 가중치를 효율적으로 조정하기 위하여, 거꾸로 무엇인가를 전파하는 방식이죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733010-15b1ec00-afd8-11ea-971e-e6b7a93655a7.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]4\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그림한번 그려볼까요?\n",
        "여기 이처럼 3개의 계층과 편향이 있는 간단한 신경망이 있습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733133-6590b300-afd8-11ea-98cf-30bfe75ae6fc.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]5\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 이 신경망은 입력값이 2일 때 입력값 3을 얻으려고 하네여.  \n",
        "가중치는 현재 초기값이니까 모두 1로 설정했다 가정하고, 계산을 한번 진행해 볼까요?  \n",
        "\n",
        "이 신경망에는 활성화 함수가 따로 없으니  \n",
        "은닉층의 첫번째 노드는 3  \n",
        "그리고 여기서도 가중치는 1이니깐  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733244-aab4e500-afd8-11ea-917d-be3355eb8957.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]6\" alt=\"RubberDuck\"></img><br/>\n",
        "최종 출력값은 4라는 결과가 나오게 됩니다.  \n",
        "\n",
        "현재 이 데이터에 대한 기대출력값은 3이지만, 가중치를 모두 1로 설정한 결과  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733324-e3ed5500-afd8-11ea-9cff-6efafd1960b6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]7\" alt=\"RubberDuck\"></img><br/>\n",
        "현재 출력값은 3보다 큰 4라는 결과가 나오게 되죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1toTF6he7Mj",
        "colab_type": "text"
      },
      "source": [
        "이제 이 가중치들을 효과적으로 조정하기 위한  \n",
        "**순전파, 역전파**를 살펴보도록 하겠습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733415-24e56980-afd9-11ea-8a9c-e96bee1dd1f6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]8\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "기대 출력값과 현재 출력값의 오차는 얼마일까요? 그렇죠 1입니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733483-4ba3a000-afd9-11ea-9d21-6c69095870e0.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]9\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "따라서 역전파에서는 기존의 출력값을 지우고 새로운 출력값으로 3을 전달합니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733576-90c7d200-afd9-11ea-8d7a-86381870ba69.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]10\" alt=\"RubberDuck\"></img><br/>\n",
        "자 아주 단편적으로 생각해 봅시다. 이러한 값이 나오기 위해서는 당연히 \n",
        "\n",
        "바로 이전의 값의 영향을 무시할 수 없겠죠?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733678-dedcd580-afd9-11ea-8147-ad393783ad09.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]11\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 바로 이전의 값에서 더 큰 값은 자연스럽게 더 큰 영향을 미치게 될 것이구요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hjlAG9Cg_jb",
        "colab_type": "text"
      },
      "source": [
        "그럼 신경망은 어떻게 할까요? 당연히 은닉층 노드값과 고정값을 비교하여  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733844-4561f380-afda-11ea-9a5a-cd6c5648d6c9.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]12\" alt=\"RubberDuck\"></img><br/>\n",
        "값이 큰쪽의 가중치 혹은 편향을 더 크게 조정하여 출력값을 3이 되도록 만들려고 하겠죠?\n",
        "\n",
        "여기서는 은닉층 고정값이 1이므로, 1/10정도 조정한다 가정하고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84733998-c1f4d200-afda-11ea-9a13-405d4b82e51a.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]13\" alt=\"RubberDuck\"></img><br/>\n",
        "편향에서 0.1을 빼 0.9로 조정합니다.  \n",
        "그리고 은닉층 노드 값은 3이므로 가중치는 3배인 0.3을 빼서 0.7로 조정했습니다.  \n",
        "\n",
        "그럼 이러한 조정을 통해 은닉픙에 전파할 오차는 총0.4가 되므로  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84734112-0d0ee500-afdb-11ea-805b-d9530318d4ec.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]14\" alt=\"RubberDuck\"></img><br/>\n",
        "역전파를 적용한 신경망의 출력값은 새로운 출력값 3에서  \n",
        "은닉층에 전파할 오차 0.4 를 빼 값인 2.6에 가까운 값이어야 합니다.  \n",
        "\n",
        "자 이러한 방식으로 입력층과 은닉증 사이의 가중치와 편향도 조정해보죠.  \n",
        "\n",
        "입력층의 편향은 0.1을 빼서 0.9로 저정하고,  \n",
        "입력값의 가중치는 0.2를 빼서 0.8로 저정합니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84734301-80b0f200-afdb-11ea-9a93-02edac7cc17c.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]15\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그럼 역전파를 통해 가중치를 계산한 결과는 이렇게 되고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84734408-bc4bbc00-afdb-11ea-9e90-bb4ad5d5acee.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]16\" alt=\"RubberDuck\"></img><br/>\n",
        "이를 다시 계산해보면 은닉층의 노드값은 2.5, 출력증의 값은 2.65  \n",
        "기대출력값에 대해, 우리가 처음에 가중치를 1로 설정하여 계산한 결과보다  \n",
        "훨씬 가까워진 결과를 확인할 수 있습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znubXCsVjsWt",
        "colab_type": "text"
      },
      "source": [
        "지금까지의 과정은 결국 신경망의 지도 데이터와 출력 값 사이의 오차로  \n",
        "가중치를 조정하는 것임을 알 수 있습니다.  \n",
        "\n",
        "즉, 역전파는 출력값과 지도 데이터 사이에 생기는 '오차'를 이용해  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84734622-5ca1e080-afdc-11ea-9937-2ccd68939f95.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]17\" alt=\"RubberDuck\"></img><br/>\n",
        "출력층에서 입력측쪽으로 가중치를 조정하는 것이죠.\n",
        "\n",
        "이러한 역전파는 경사하강법을 사용하는 것이기도 해요.  \n",
        "\n",
        "이와 같은 신경망에서의 손실함수를 정의해 보면,  \n",
        "손실함수는 출력값과 지도 데이터 사이의 오차라고 할 수 있습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84734828-ec478f00-afdc-11ea-92f2-eba1588442ed.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]18\" alt=\"RubberDuck\"></img><br/>\n",
        "그럼 출력값이 z1, z2고 지도데이터가 t1, t2이므로  \n",
        "손실함수식은 이렇게 쓸수 있어요.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_N-xFoleeA",
        "colab_type": "text"
      },
      "source": [
        "역전파는 이 손실함수가 최소값일 때의 가중치로 원래의 가중치를 조정해야 합니다.  \n",
        "그래서 입력값 각각의 손실함수 전체를 고려해야 하죠.  \n",
        "이어지는 이야기로, 특정 입력값에서의 손실 함수값이 최소가 되더라도  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735008-5ceeab80-afdd-11ea-8fb4-96abb191ed09.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]19\" alt=\"RubberDuck\"></img><br/>\n",
        "전체를 생각했을때의 의미는 크게 없습니다.  \n",
        "\n",
        "궁극적인 목표는 모든 입력값을 대상으로 손실함수가 최소값일때의 파라미터를 찾는 것이죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735334-38470380-afde-11ea-9d8a-65a6c3cab176.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]20\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그럼 입력값 각각의 손실함수 합은 이렇게 표현할 수 있겠네요.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735544-c28f6780-afde-11ea-8b95-162c401a0f22.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]21\" alt=\"RubberDuck\"></img><br/>\n",
        "이때 손실함수 E가 최소값일 때의 가중치를 찾는 것은  \n",
        "이 식처럼 E를 가중치 W에대해 편미분하는 것이 되겠습니다.  \n",
        "그리고 우린 미분한 값이 0에 가까운 가중치를 찾아야하는 것이죠.  \n",
        "0에 가까운 미분값을 찾는 이유는 이전에 살펴봤던 경사하강법에서의  \n",
        "기울기 값이 0에 가까워졌을때, 손실함수값이 최소값 후보가 되기 때문입니다.  \n",
        "\n",
        "자 여기서 후보라는 단어를 사용했는데, 계산 결과를 통해 기울기가 0에 가까워졌다고 해도  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735651-ff5b5e80-afde-11ea-8f2d-61a393343f62.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]22\" alt=\"RubberDuck\"></img><br/>\n",
        "그 값이 꼭 최소값이라 확신할 순 없습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydq_0gvfpRI1",
        "colab_type": "text"
      },
      "source": [
        "그 이유인 즉,  \n",
        "기울기에 대한 변화가 없다가 다시 점차 증가 혹은 감소하는 현상이 발생할 수도 있으며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735797-4f3a2580-afdf-11ea-812f-12c7730c423d.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]23\" alt=\"RubberDuck\"></img><br/>\n",
        "마치 심하게 요동치는 파동과도 같은 모습을 보이는 경우가 다수 있기 때문입니다.  \n",
        "\n",
        "그렇게에 기울기가 0에 가깝게 수렴했을 때, 무작정 최소값이라 단정짓는 것은  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735901-92949400-afdf-11ea-9796-dcf289d8d01f.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]24\" alt=\"RubberDuck\"></img><br/>\n",
        "꽤나 위험한 행동이 될 수 있죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGYvbAeCppQK",
        "colab_type": "text"
      },
      "source": [
        "다시 돌아와서  \n",
        "이러한 식에서 E를 각각 전개하여 직접 편미분을 진행하면 괘나 번거롭습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84735993-cbcd0400-afdf-11ea-9867-1721d823efa2.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]25\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그렇기에 입력값 각각의 손실함수를 편미분한 후에 합이 0에 가까운지를 확인하는 것이 더 간단하죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84736125-16e71700-afe0-11ea-8211-ef5ef01f6937.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]26\" alt=\"RubberDuck\"></img><br/>\n",
        "그럼 손실함수가 최소값일때의 가중치를 찾아보도록 하겠습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-1K90-AqiXa",
        "colab_type": "text"
      },
      "source": [
        "이렇게 지도 데이터가 1개인 신경망에서 손실함수가 최소값일 때의  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84736230-57df2b80-afe0-11ea-9c08-2a5c7306ac4c.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]27\" alt=\"RubberDuck\"></img><br/>\n",
        "가중치를 찾는 과정을 살펴보겠습니다.  \n",
        "\n",
        "은닉층과 출력층 사이에서  \n",
        "은닉층 가중치가 w, 은닉층 변향이 b라면  \n",
        "실제 출력값은 z = b + wy가 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84736512-f4a1c900-afe0-11ea-9ad1-aef4d5f643ca.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]28\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 z의 손실함수는 이렇게(E=(z-t)^2) 되고요  \n",
        "E에 관해 w로 편미분 할 때는 너무복잡하니 연쇄법칙을 이용하여  \n",
        "간단하게 할 필요가 있는데,  \n",
        "\n",
        "먼저 b로 편미분을 진행해줍니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84736619-33d01a00-afe1-11ea-8136-d23a2ccd2043.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]29\" alt=\"RubberDuck\"></img><br/>\n",
        "그 다음으로 E에 관해 w로 편미분하게 되면 이러한 결과를 얻을 수 있죠. \n",
        "\n",
        "이렇게 되면 은닉층 편향의 편미분으로 E에 관한 은닉층 가중치 w의 편미분을 정의할 수 있게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84736867-b48f1600-afe1-11ea-9bb9-1b532e2fe201.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]30\" alt=\"RubberDuck\"></img><br/>\n",
        "최종적으로 이 실을 통해 이 은닉층 가중치는  \n",
        "은닉층 편향의 변화량에 y를 곱한 값을 빼서 은닉층 가중치를 조정할 수 있게 되는 겂니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFKm-CFBtytF",
        "colab_type": "text"
      },
      "source": [
        "이와 같은 방식으로 입력층과 은닉층 사이도 가중치와 편향을 조정할 수 있죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84737255-7ba37100-afe2-11ea-9d7d-31adffd71530.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]31\" alt=\"RubberDuck\"></img><br/>\n",
        "입력값의 가중치가v, 입력층의 편향이a라면  \n",
        "은닉층 노드값은 y = a + vx 라고 할 수 있죠.  \n",
        "\n",
        "따라서 E를 v에 관해 편미분 할 때고 연쇄 법칙을 적용하게 되면  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84737681-67ac3f00-afe3-11ea-9c35-cb3863750ae7.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]32\" alt=\"RubberDuck\"></img><br/>\n",
        "a와 입력값x의 관계로 정의할 수 있게 됩니다.  \n",
        "즉, 입력값의 가중치도 입력층 고정값이 1일 때,  \n",
        "입력층 편향의 변화량 곱하기 x 만큼 빼서 조정할 수 있게 됩니다.  \n",
        "\n",
        "이렇게 역전파를 활용하게 되면, 신경망의 출력값부터 차례로 이전 노드값이 정해지게 되며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84738092-3a13c580-afe4-11ea-99fa-828a7da251ea.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]33\" alt=\"RubberDuck\"></img><br/>\n",
        "이러한 역전파는 수열의 점화식처럼 오차를 전파하는 방법이기도 합니다.  \n",
        "역전파를 때로는 역방향미분 이라고 하기도 하는데  \n",
        "그만큼 역전파에 있어서 미분과정은 매우 중요하게 여겨지고 있습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIaqFzuxS66",
        "colab_type": "text"
      },
      "source": [
        "그럼 우리는 이러한 역전파를 매번 고생해서 수식을 코드로 옮겨와야 하는걸까요?  \n",
        "다행이도 텐서플로우와 더블어 다양한 라이브러리들을 활용하게 되면  \n",
        "단 한줄의 코드만으로도 이러한 역전파를 쉽게 구현할 수 있게 됩니다.  \n",
        "그렇기에 우린 이러한 역전파 알고리즘 공식을 도출하는데  \n",
        "시간을 낭비할 필요가 없으며 그저 경사를 계산해   \n",
        "최적화하는 방식을 잘 이해하고 넘어가 주시면 됩니다.  \n",
        "\n",
        "어찌되었던 우리가 살펴본 역전파 기법은  \n",
        "인공지능 연구에 있어 제2의 신호탄과 다음 없었죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhciHBclyG3D",
        "colab_type": "text"
      },
      "source": [
        "###기울기 소멸  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84738470-ece42380-afe4-11ea-82ab-b34c77a91541.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]34\" alt=\"RubberDuck\"></img><br/>\n",
        "하지만 이 역전파 기법에도 작은 문제가 있었으니, 바로 기울기 소멸 문제였죠.  \n",
        "\n",
        "당시 역전파 알고리즘으로 학습을 진행하는데 있어,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84738608-2fa5fb80-afe5-11ea-96c0-7b8b2ca2bed7.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]35\" alt=\"RubberDuck\"></img><br/>\n",
        "주로 사용된 활성화 함수는 시그모이드와 소프트맥스 였습니다.  \n",
        "\n",
        "우선 시그모이드의 경우 미분의 초대치가 0.3에 불과하며, 여러층을 거칠 수록  \n",
        "기울기는 점차 0에 수렴하게 되는 문제가 발생하였고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84738686-519f7e00-afe5-11ea-87ee-9763bf674677.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]36\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "소프트맥스는 출력값으로 부터 활률 벡터를 얻기 위해 사용되었는데,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84738828-8c091b00-afe5-11ea-90b5-23a2eef18550.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]37\" alt=\"RubberDuck\"></img><br/>\n",
        "각 출력 노드의 출력값을 0에서 1사이의 값으로 제한하였죠.  \n",
        "\n",
        "시그모이드나, 소프트맥스는 최종출력을 결정하는데 있어 합리적인 선택이 가능했으나,  \n",
        "출력된 값들이 항상 너무 작은 값을 가지고 있었기에 신경망이 깊어지면 깊어질수록  \n",
        "오차의 기울기가 점차 작아지며, 끝으로 가는 도중 기울기가 소실돼버리면서  \n",
        "가중치 조정이 이뤄지지 않는다는 문제가 발생하게 됩니다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfrFp6cRz8q5",
        "colab_type": "text"
      },
      "source": [
        "이러한 기울기 소멸 문제를 해결하기 위해제프리 힌튼 교수는  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84739368-7516f880-afe6-11ea-8e7b-c23ee4565038.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[2]38\" alt=\"RubberDuck\"></img><br/>\n",
        "다양한 활성화 함수를 제시하였으며  \n",
        "그중 **렐루(ReLU)**라는 활성화 함수르 활용하게 되면  \n",
        "어느 정도는 문제가 해결됨을 발견하였죠.  \n",
        "이 활성화 함수는 입력이 음수일 때는 0을 출력하지만,  \n",
        "양수일 때는 양수값을 그대로 출력하기에  \n",
        "다른 활성화 함수보다 기울기 소실 문제에 있어 어느 정도 면역을 갖게 됩니다.  \n",
        "\n",
        "인공신경망은 아직도 전 세계의 많은 국가와 기업  \n",
        "그리고 학자들에 의해 연구가 진행되고 있으며  \n",
        "다양한 계선방향에 대한 연구물들이 계속해서 나오고 있느 겁니다.  \n",
        "오늘의 수업은 여기서 마무리...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT99OKPkBlRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
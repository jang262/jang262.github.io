{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4주차2강_딥러닝_원리[1].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSaaN5HRNcGitMfGqSD+Dn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jang262/jang262.github.io/blob/master/4%EC%A3%BC%EC%B0%A82%EA%B0%95_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9B%90%EB%A6%AC%5B1%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsw-xPyM4-DQ",
        "colab_type": "text"
      },
      "source": [
        "딥러닝 그리고 인공신경망과 관련된 알고리즘을 살펴보기 위해서는  \n",
        "초창기 머신러닝을 간단히 살펴볼 필요가있습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84724351-ee045900-afc2-11ea-9d8e-ab384790ddfc.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]1\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "AI를 설계하기 위해, 생물학적뇌가 동작하는 방식을 이해하려는 시도로,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84724523-5d7a4880-afc3-11ea-95f0-8685abb21ad5.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]2\" alt=\"RubberDuck\"></img><br/>\n",
        "1943년 워랜 맥컬록과 월터 피츠는 처음으로 간소화된 뇌의 뉴런 개념을 발표했습니다.  \n",
        "이를 맥컬록-피츠 뉴런(MCP)이라 하죠.  \n",
        "\n",
        "우선 뉴런들은 지난 시간에도 살펴봤듯이 뇌의 신경세표와 서로 연결되어 있으며  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84724688-c06bdf80-afc3-11ea-8a32-db54b6dc1155.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]3\" alt=\"RubberDuck\"></img><br/>\n",
        "화학적, 전기적 신호를 처리하고 전달하는데 관여합니다.  \n",
        "맥컬록과 피츠는 이러한 신경세포를 이진 출력을 내는 간단한 논리 회로로 표현했으며,  \n",
        "\n",
        "몇년 후 프랭크 로젠 블렛은 MCP 뉴런 모델을 기반으로  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84724784-f610c880-afc3-11ea-8aaa-746f81812e83.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]4\" alt=\"RubberDuck\"></img><br/>\n",
        "퍼셉트론 학습 개념을 처음 발표하게 되죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWYPsyP_9Elq",
        "colab_type": "text"
      },
      "source": [
        "퍼셉트론 규칙에서 프랭크 로젠 블렛은 자동으로 최적의 가중치를 학습하는  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725077-bbf3f680-afc4-11ea-960a-3aa486749d7b.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]5\" alt=\"RubberDuck\"></img><br/>\n",
        "알고리즘을 제안합니다.  \n",
        "이 가중치는 뉴런의 출력 신호를 낼지 말지를 결정하기 위해 입력 특성에 곱하는 계수가 되죠.  \n",
        "\n",
        "여기서 이 가중치의 개념을 좀 더 자세하게 살펴보면,  \n",
        "1949년 캐나다의 심리학자 도날드 헵  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725234-19884300-afc5-11ea-8c59-4eb5065a4db7.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]6\" alt=\"RubberDuck\"></img><br/>\n",
        "그는 인간 두뇌의 작용은 개별 신경세포에 의해서  \n",
        "이뤄지는 것이 아닌, 그들 간의 연결 강도에 의해서 정해진다는 연결주의를 주장하며  \n",
        "우리의 두뇌가 신경망으로 활동하고 있음을 설명하였습니다.  \n",
        "\n",
        "그리고 이러한 이론에 자신의 이름을 붙여 헵의 학습규칙이라 부르게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725298-4177a680-afc5-11ea-8e1f-33d1bc8715bc.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]7\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이규칙은 가장 오래되고 단순한 형태인데, 만약 시냅스가  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725456-a29f7a00-afc5-11ea-8bbc-0675d200043a.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]8\" alt=\"RubberDuck\"></img><br/>\n",
        "양쪽 뉴런이 동시에 또 반복적으로 활성화되었다면 그 뉴런 사이의 연결 강도가  \n",
        "강화된다는 관찰에 근거하죠.  \n",
        "그리고 이러한 헵의 학습규칙은 이후 신경망 모델들의 학습규칙에 토대가 됩니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeTaiuwoBX4s",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "이러한 퍼셉트론을 간단하게 표현해보면 입력을 받아서 계산 후  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725586-f4480480-afc5-11ea-8e00-dadf4b37a7f6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]9\" alt=\"RubberDuck\"></img><br/>\n",
        "출력을 반환하는 구조로 나타낼 수 있는데, 사실 신경망은  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725703-29eced80-afc6-11ea-9beb-3f839ad4ffd8.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]10\" alt=\"RubberDuck\"></img><br/>\n",
        "뉴런이 여러개 모여 레이어를 구성한 후, 이 레이어들이 다시 모여 구성된 형태입니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725822-76d0c400-afc6-11ea-8c55-be5f4767ae29.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]11\" alt=\"RubberDuck\"></img><br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpEMrAtDBeoq",
        "colab_type": "text"
      },
      "source": [
        "그리고 이 하나의 뉴런을 좀더 자세하게 표현하면  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725880-9ec02780-afc6-11ea-92a4-6a882605a4bb.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]12\" alt=\"RubberDuck\"></img><br/>\n",
        "이렇게 가중치와 활성화 함수가 숨어있는 것을 확인할 수 있죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84725958-c616f480-afc6-11ea-8103-c8880b8c225c.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]13\" alt=\"RubberDuck\"></img><br/>\n",
        "이때 이 활성화 함수는 뉴런의 출력값을 정하는 함수로서  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84726047-f363a280-afc6-11ea-8f67-69a7c0b82e92.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]14\" alt=\"RubberDuck\"></img><br/>\n",
        "가장 간단한 형태의 뉴런은 입력에 가중치를 곱한 뒤,  \n",
        "활성화 함수를 취하면 출력 값을 얻을 수 있습니다.  \n",
        "\n",
        "그리고 이 뉴런에서 학습할때 변하는 것은 가중치입니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84726343-a16f4c80-afc7-11ea-8bd3-6f9e553eb7ab.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]15\" alt=\"RubberDuck\"></img><br/>\n",
        "가중치는 처음에 초기화를 통해 무작위된 값을 넣고,  \n",
        "학습 과정에서 점차 일정한 값으로 수렴하게 돼요.  \n",
        "그럼 학습이 잘 된다는 것은, 좋은 가중치를 얻어 원하는 출력값에  \n",
        "점점 가까워지는 값을 얻는 것이라 할 수 있겠죠?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9t42A-3Cyy6",
        "colab_type": "text"
      },
      "source": [
        "그럼 파이썬을 통해 간단하게 실험을 해보면서  \n",
        "입력값이 1일때 우리가 원하는 출력값은 0으로 설정하고  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84726608-3d00bd00-afc8-11ea-9723-41495212ff02.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]16\" alt=\"RubberDuck\"></img><br/>\n",
        "가중치는 정규분포의 무작위 값을 설정  \n",
        "그리고 활성화 함수에는 여러가지가 있는데  \n",
        "이건 이후 강의에서 살펴보도록 하고  \n",
        "오늘은 그중에서 시그모이드라는 아주 유명한 활성화 함수를 사용해 볼꺼에요\n",
        "\n",
        "그럼 이렇게 입력과 가중치가 곱해져 활성화함수를 통과하게 되었을때   \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84726779-95d05580-afc8-11ea-9e38-a28f39a07dd1.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]17\" alt=\"RubberDuck\"></img><br/>\n",
        "출력값을 확인해보면  \n",
        "생각보다 0과의 거리가 먼 값이 출력되죠.  \n",
        "\n",
        "그 이유는 가중치를 무작위로 선정했고  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84726946-f3fd3880-afc8-11ea-8eb0-8dbe287dc1c4.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]18\" alt=\"RubberDuck\"></img><br/>\n",
        "이 가중치는 우리가 원하는 출력값을 얻기에  \n",
        "적합한 가중치가 아님을 의미합니다.  \n",
        "\n",
        "그리고 우리가 기대한 출력값인 0과 최종 출력된 이 값의 차이를  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84727107-4a6a7700-afc9-11ea-868b-50a57096f6cc.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]19\" alt=\"RubberDuck\"></img><br/>\n",
        "error라고 하며 이 error가 0에 가까워지도록  \n",
        "가중치를 조절해 줘야하고 이러한 조절 방식에는 경사하강법 이라는 기법이 사용되죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvkdvSckF5Dk",
        "colab_type": "text"
      },
      "source": [
        "자그럼 다시 천천히 살펴보면  \n",
        "우선 에러값을 생성하기 위해  \n",
        "실제 결과값에서 예측 결과값의 차이를 저장할 수있는  \n",
        "에러 객체를 생성해주고  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84727218-8ef61280-afc9-11ea-89f0-bdde647e14a1.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]20\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "**경사하강법**을 코드로 구현해 줘야 하는데  \n",
        "이 가중치 갱신을 위해 **학습률**이라는 개념이 등장하게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84727547-3f641680-afca-11ea-8dd3-758f22cda840.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]21\" alt=\"RubberDuck\"></img><br/>\n",
        "학습률이란 가중치를 어느정도의 크기로 조정할 것이냐를 물어보는 하이퍼 파라미터인데  \n",
        "\n",
        "간단하게 표기하면 기존 가중치에 입력값과 학습률그리고 에러값을 곱한값을 더해주는 것이죠.  \n",
        "\n",
        "보편적으로 많이 사용되는 값은 적당히 작은 값이구요.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktnNgsSFHIkx",
        "colab_type": "text"
      },
      "source": [
        "그럼 이 경사하강법이 제대로 효과를 발휘하는지 코드로 확인해보도록 하겠습니다.  \n",
        "\n",
        "실제로 가중치의 조정을 1000번간 진행 했으며결과로 나온값을 살펴보면  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728074-6707ae80-afcb-11ea-901e-0fac0ff46500.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]23\" alt=\"RubberDuck\"></img><br/>\n",
        "예상 출력값과 에러값 모두 우리가 원하는값으로  \n",
        "수렴하고 있습니다.  \n",
        "\n",
        "그럼 반대로 입력값을 0으로하고 출력값을 1로 설정했을 때는 어떻게 될까요?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728397-16448580-afcc-11ea-9064-f7a51e3605e0.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]23\" alt=\"RubberDuck\"></img><br/>\n",
        "가중치 조정 횟수, 학습률 모두 동일하게 설정하고 진행해 볼께요.  \n",
        "\n",
        "이때는 우리가 원하는 출력값이 아닌  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728447-38d69e80-afcc-11ea-8ff3-bb2bac795741.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]24\" alt=\"RubberDuck\"></img><br/>\n",
        "0.5라는 값에서 변하지 않고 있습니다.  \n",
        "\n",
        "왜 그렇까요? 자 경사하강법의 식을 자세하게 살펴보면  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728644-9f5bbc80-afcc-11ea-8be3-9a89b95d4df9.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]25\" alt=\"RubberDuck\"></img><br/>\n",
        "입력값에 학습률을 곱해주는 부분이 있습니다.  \n",
        "그렇단 말은 입력값에 0이 들어오는 순간부터  \n",
        "그 어떠한 학습률값을 넣어도 이후 가중치에 더해지는 값이 없다는 것이죠.  \n",
        "즉, 1000번의 가중치 조정은 의미가 없었다는 겁니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zh5w31sKrja",
        "colab_type": "text"
      },
      "source": [
        "그럼 어떻게 해야 할까요?  \n",
        "이러한 경우를 방지하고자 **편향**이란 개념이 등장하며 이 편향은  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728801-11cc9c80-afcd-11ea-869c-5151341db6b6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]26\" alt=\"RubberDuck\"></img><br/>\n",
        "의미 그대로 입력으로는 늘 한쪽으로 치우처진 고정 값이며,  \n",
        "입력으로 받은 값이 0인 경우에 아무것도 학습하지 못하는것을 방지해주죠.  \n",
        "\n",
        "이러한 편향 값도 가중치처럼 난수로 초기화되며  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728876-43ddfe80-afcd-11ea-9914-2c72d55e2132.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]27\" alt=\"RubberDuck\"></img><br/>\n",
        "뉴런에 더해져 출력을 계산하게 됩니다.  \n",
        "\n",
        "코드로 확인해 보면, 편향을 추가한것을 확인할 수있고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84728981-8bfd2100-afcd-11ea-80b1-78ed4c6556e1.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]28\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이러한 가중치들이 반복을 통해 업데이트되면서, 우리가 원하는 결과값에  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729082-bfd84680-afcd-11ea-8a05-8929740a578e.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]29\" alt=\"RubberDuck\"></img><br/>\n",
        "수렴하는 값을 출력합니다.  \n",
        "이로서 입력이 0인 상황 속에서도 학습이 잘 되는것을 확인할 수있네요.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYCdU22MpJ7",
        "colab_type": "text"
      },
      "source": [
        "사실 이러한 퍼셉트론을 잘 이해하는 방법은  \n",
        "바로 인공지능의 겨울을 불러왔던  \n",
        "AND OR XOR 연산문제를 다뤄보는 겁니다.  \n",
        "이 세가지 연산은 아주 간단합니다.  \n",
        "\n",
        "우선 and연산의 경우 두개의 입력값 참 혹은 거짓에 대하여  \n",
        "모두 참인 경우에만 참을 출력하고  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729290-4ee55e80-afce-11ea-939b-1b6bf1768729.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]30\" alt=\"RubberDuck\"></img><br/>\n",
        "나머지 경우에 대해서는 모두 거짓을 출력합니다.  \n",
        "\n",
        "이를 정수로 치환하여 그래프로 그려보면 대략 이렇게 그려볼 수 있죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729355-76d4c200-afce-11ea-9c12-5abff7f54b81.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]31\" alt=\"RubberDuck\"></img><br/>\n",
        "이때 이 참과 거짓을 하나의 선으로 구분하기 위해서는 어떻게 하면 될까요?\n",
        "\n",
        "대략 이렇게 선을 그어주면 참과 거짓을 구분할 수 있게 되죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729501-d337e180-afce-11ea-9424-0f189e2a45fa.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]32\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "페셉트론 또한 이 and연산에 대해서는 무리 없이 각 진리표에 대해  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729582-0e3a1500-afcf-11ea-9af2-8a446143bd0e.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]33\" alt=\"RubberDuck\"></img><br/>\n",
        "기대 출력과 가까운 값을 출력하게 됩니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXsSRVUdPLKT",
        "colab_type": "text"
      },
      "source": [
        "다음 or연산은 and연산과는 반대로, 모든 입력값이 거짓일 때만 거짓을 출력하고  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729713-51948380-afcf-11ea-9738-f8ae8b54960f.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]34\" alt=\"RubberDuck\"></img><br/>\n",
        "나머지 입력에 대해서는 모두 참을 출력 합니다.  \n",
        "\n",
        "이것도 정수로 치환하여 그래프로 살펴보면 대략 이렇게 그려볼 수 있죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729831-a33d0e00-afcf-11ea-877e-1dd6ec4d8dd3.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]35\" alt=\"RubberDuck\"></img><br/>\n",
        "그럼 이 그래프에서조 하나의 선을 활용하여 참과 거짓을 구분할 수 있을까요?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84729953-e5fee600-afcf-11ea-9a36-4e2235aa3505.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]36\" alt=\"RubberDuck\"></img><br/>\\\n",
        "당연히 가능하겠죠  \n",
        "\n",
        "페셉트론도 or연산은 무리없이 진리표에 대해  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730026-16df1b00-afd0-11ea-8f94-af706dfadb27.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]37\" alt=\"RubberDuck\"></img><br/>\n",
        "기대 출력과 가까운 값을 출력하게 됩니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCkgNIr2REsZ",
        "colab_type": "text"
      },
      "source": [
        "자 그럼 xor문제를 자세하게 살펴보죠.  \n",
        "\n",
        "이 xor은 간단하게 두개의 입력값이 서로 다를때 참을 출력하게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730127-573e9900-afd0-11ea-895f-5ed7e1184299.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]38\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그럼 이러한 진리표를 정수로 표현해보면 이렇게 표현할 수 있고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730154-66bde200-afd0-11ea-9381-d03454c370c6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]39\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이를 그래프로 그려보면 대략 이러한 그래프를 그려볼 수 있습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730228-8d7c1880-afd0-11ea-8359-b2082e9f4ff6.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]40\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이때 이 참과 거짓을 하나의 직선으로 구분할 수 있을 까요?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730287-bc928a00-afd0-11ea-978c-d4beedb990ff.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]41\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "퍼셉트론 또한 이러한 xor연산에 대해 다양한 가중치를 바탕으로 학습을 진행해도  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730382-f9f71780-afd0-11ea-9f11-1aabf5e79180.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]42\" alt=\"RubberDuck\"></img><br/>\n",
        "모든 결과값이 0.5 근처에 머물며 우리가 원하는 값으로는 수렴하지 않았습니다.  \n",
        "\n",
        "음 그럼 해결책은 무엇이었을 까요?  \n",
        "\n",
        "간단하게도 단층이 아닌, **다층 퍼셉트론**을 활용하게 되면 어느 정도는 문제가 해결된다는 것이었죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730479-37f43b80-afd1-11ea-855b-fcc38cc6b791.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]43\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "하지만 1969년 당시 연구진들은 이렇게 간단한 xor문제 조차 풀 수 없는 퍼셉트론을  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730583-7e499a80-afd1-11ea-9340-9f2e8d0b3d4a.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]44\" alt=\"RubberDuck\"></img><br/>\n",
        "그저 단순 선형 분류기에 불과하다 하였고, 인간의 뇌를 모방한 퍼셉트론의 한계는  \n",
        "곧 인공신경망 연구의 겨울을 불러오게 되었습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLTHuRCFSpQT",
        "colab_type": "text"
      },
      "source": [
        "물론 저서 퍼셉트론에서도 다층 퍼셉트론을 사용하게 되면 어느정도 문제는  \n",
        "해결된다 언급은 하였지만, 크게 받아들여지지는 않았습니다.  \n",
        "\n",
        "다시 돌아와 시간이 조금 흐른뒤 1986년  \n",
        "다수의 연구진들은 이 저서를 통해 다층퍼셉트론을 제시하였는데  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730744-09c32b80-afd2-11ea-95e7-59363bf3b5fe.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]45\" alt=\"RubberDuck\"></img><br/>\n",
        "이 저서에서는 은닉층을 활용하게 되면, 선형분류 판별선을  \n",
        "여러개 그리는 효과를 얻음으로써 xor문제를 해결할 수 있다 말하였죠.  \n",
        "\n",
        "하지만 이 다층 퍼셉트론의 치명적인 약점은 바로 파라미터 개수가 많아지면서  \n",
        "적절한 가중치와 편향을 학습하는 것이 어렵다는 것이었는데,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84730985-7807ee00-afd2-11ea-935a-a5fad6417597.png\" width=\"450px\" height=\"300px\" title=\"딥러닝_원리[1]46\" alt=\"RubberDuck\"></img><br/>\n",
        "이부분에 대하여 제프리 힌튼은 **역전파 알고리즘**을 제시하며 문제르 깔끔하게 해결하게 됩니다.  \n",
        "\n",
        "이후 다층 퍼셉트론 즉, 딥러닝의 뿌리가 되는 이 기법은  \n",
        "인공지능 연구를 가속화시켜주는 계기가 되었으며  \n",
        "이 뒤에는 묵묵하게 인공신경망의 힘을 믿고 꾸준하게 연구에 몰두한  \n",
        "연구진들이 있었다는 사실을 기억해 주셨으면 합니다.  \n",
        "결과론적인 이야기지만 잠시 세상을 둘러보면  \n",
        "믿음이란 힘 그리고 수많은 학자들의 노력은 결코 헛되지 않았음을 알수있죠.  \n",
        "즉, 우리의 믿음 우리의 노력으로 지금 당장 꽃피울 수는 없겠지만  \n",
        "인공지능의 겨울이 막을 내리듯  \n",
        "우리의 믿음과 노력 또하 결코 헛되지 않을 것입니다.  \n",
        "오늘의 수업은 여기서 마무리...  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT99OKPkBlRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
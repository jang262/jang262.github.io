{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4주차4강_강화학습[1].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR3xOlx9XSM+QBHC8i6qFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jang262/jang262.github.io/blob/master/4%EC%A3%BC%EC%B0%A84%EA%B0%95_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%5B1%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmGPAxaH1itF",
        "colab_type": "text"
      },
      "source": [
        "# 강화학습(Reinforcement Learning)  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84741423-105d9d00-afea-11ea-8b32-80cf13e8ad42.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]1\" alt=\"RubberDuck\"></img><br/>\n",
        "강화학습은 지도 학습처럼 정답이 있지도 않고,  \n",
        "비지도 학습처럼 데이터만을 기반으로 학습하지도 않습니다.  \n",
        "\n",
        "강화학습은 에이전트라는 존재가 환경과 상호작용하며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84741696-67637200-afea-11ea-8bc5-bde712e70bbc.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]2\" alt=\"RubberDuck\"></img><br/>\n",
        "이 환경에는 보상이라는 기준이 있어서 다양한 시행착오를 겪어가며  \n",
        "보상을 최대화하는 방향으로 학습을 진행하죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zai0RNpF8Llq",
        "colab_type": "text"
      },
      "source": [
        "잠시 과거를 조금 회상해 볼까요?  \n",
        "우린 지금부터 자전거를 처음타는 시절로 돌아간 겁니다.  \n",
        "\n",
        "당연히 자전거는 처음이니 여러번 넘어지고, 허벅지도 아프고, 힘이 많이 듭니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84741905-b4dfdf00-afea-11ea-9068-dbeb05ecfe1e.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]3\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "강화학습 측면에서 이러한 부분은 '페널티를 받는다.'라고 할 수 있습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84742042-e0fb6000-afea-11ea-966b-c980781dd933.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]4\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그러다가 1~2시간쯤 지나고 나서, 이제 넘어지지 않고 잘 달리게 되었습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84742217-215ade00-afeb-11ea-90c8-4252a981b7d2.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]5\" alt=\"RubberDuck\"></img><br/>\n",
        "그럼 페널티와는 반대로 이는 '보상을 받는다.'라고 할 수 있습니다.  \n",
        "\n",
        "이렇게 여러번의 시도를 거쳐 점점 넘어지지 않게 학습하게 되고,  \n",
        "결국 우린 자연그럽게 자전거를 잘 타는 방법을 학습하게 되죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84742447-67b03d00-afeb-11ea-9834-351a36a2d2db.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]6\" alt=\"RubberDuck\"></img><br/>\n",
        "이것이 강화학습의 학습과정에 대한 예시입니다.  \n",
        "\n",
        "강화학습은 이렇게 다양한 시행착오를 통해 학습이 가능하며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84742586-9f1ee980-afeb-11ea-991f-3e42a84e53f8.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]7\" alt=\"RubberDuck\"></img><br/>\n",
        "비교적 명확한 보상을 설정할 수 있는 문제를 해결하는데 사용되고 있어요\n",
        "\n",
        "이렇게 강화학습은 빠르게 발전을 거듭해 오면서 지금껏 인공지능으로  \n",
        "해결하기 힘들다고 생각한 많은 문제들을 해결해 왔죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZtK8qgS_AqQ",
        "colab_type": "text"
      },
      "source": [
        "2016년 3월 전 세계를 놀라게 한 구글 딥마인드의 알파고와  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84742846-13f22380-afec-11ea-9902-b606d0fbce2d.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]8\" alt=\"RubberDuck\"></img><br/>\n",
        "세계 정상급 프로기사 이세돌 9단의 대국은  \n",
        "인공지능 역사에 있어 한 획을 긋게 됩니다.  \n",
        "\n",
        "바둑의 모든 경우의 수를 계산해 보면 361!(팩토이얼)로,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84743027-4ef45700-afec-11ea-8204-0cd71df2efe0.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]9\" alt=\"RubberDuck\"></img><br/>\n",
        "이는 우주 전체의 원자 수 보다 많은 경우의 수라 합니다.  \n",
        "\n",
        "또한 바둑은 계산적인 선택 외에도 간혹 인간의 감각적인 측면과도 같은  \n",
        "아직까지 설명이 불가능한 그 무엇인가에 의해 승패의 당락이 결정지어지기도 하죠.  \n",
        "\n",
        "결과적으로 이 대국에서는 알파고가 승리를 가져 갔으며 이를 시작으로  \n",
        "인공지능 그리고 강화학습 분야가 많은 관심을 받게 되었습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERKb98cp_Dom",
        "colab_type": "text"
      },
      "source": [
        "그럼 도대체 강화학습은 어떠한 알고리즘에 의해 다치고, 넘어지며  \n",
        "때로는 뿌듯해하는 걸까요?  \n",
        "\n",
        "강화학습을 한문장으로 정리해 보면  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84743461-f6718980-afec-11ea-9304-6bb29256846a.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]10\" alt=\"RubberDuck\"></img><br/>\n",
        "보상을 최대화 하는 의사결정전략  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84743552-1739df00-afed-11ea-9f13-ad67a7f43ef8.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]11\" alt=\"RubberDuck\"></img><br/>\n",
        "즉, 순차적인 행동을 알아나가는 방법입니다.  \n",
        "\n",
        "여기서 순차적으로 계속 행동을 결정해야하는 문제를 수학적으로 정의한 것이  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84743769-5bc57a80-afed-11ea-9b0d-9b4b4ff7f690.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]12\" alt=\"RubberDuck\"></img><br/>\n",
        "그 유명한 **Markov Decision Process**입니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWWZXTUAPW2",
        "colab_type": "text"
      },
      "source": [
        "<u>**MDP**는 상태, 행동, 보상함수, 상태변환확률, 감사율</u>이라는  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84744033-afd05f00-afed-11ea-8071-ab31f089a24e.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]13\" alt=\"RubberDuck\"></img><br/>\n",
        "구성요소들을 바탕으로 이뤄져 있는데, 하나씩 살펴보도록 하죠.  \n",
        "그리 어렵지는 않습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmhXebkA9ez",
        "colab_type": "text"
      },
      "source": [
        "우선 <u>**에이전트**는 강화학습에서 의사결정을 하는 역할</u>을 맡고 있어요.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84744339-0e95d880-afee-11ea-82bb-f8dbd578bb27.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]14\" alt=\"RubberDuck\"></img><br/>\n",
        "예를 들어 게임을 할때 우리가 조종하는 게임안의 주인공이라 생각하면 됩니다.  \n",
        "\n",
        "다음 <u>**환경**은 에이전트의 의사결정을 반영하며, 에이전트에세 반영된 정보를 주는 역할</u>을 담당합니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84744528-59afeb80-afee-11ea-905d-38d5bf81a040.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]15\" alt=\"RubberDuck\"></img><br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bClEkikCvti",
        "colab_type": "text"
      },
      "source": [
        "다음은 **상태**.  \n",
        "에이전트는 상태라는 것을 기반으로 의사결정을 진행하게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84745089-09855900-afef-11ea-91f7-0f3a6b7a80f4.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]16\" alt=\"RubberDuck\"></img><br/>\n",
        "이 <u>상태라는 요소는 의사결정을 하기 위해 사용되는 관측값, 행동, 보상을 가공한 정보</u>라 할 수 있죠.  \n",
        "\n",
        "다음 <u>**행동**은 에이전트가 의사결정을 통해 취할 수 있는 행동을 의미하는데,  \n",
        "일반적으로 현재 상태에서 취하는 행동을  \n",
        "A<sub>t</sub>라고 표현합니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84745669-dbecdf80-afef-11ea-95d2-bd85c6e0661c.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]17\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 이 행동에는 이산적인 행동과 연속적인 행동이 있죠.  \n",
        "이러한 행동은 환경에 따라 정해지게 됩니다.</u>  \n",
        "\n",
        "이산적인 행동을하는 환경은 에이전트에게 주어지는 행동의 선택지가 있으며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84745952-4140d080-aff0-11ea-9907-2a35644b2ff6.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]18\" alt=\"RubberDuck\"></img><br/>\n",
        "에이전트는 그중 하나를 선택하게 됩니다.  \n",
        "\n",
        "연속적인 행동을 하는 환경같은 경우에는 선택지마다 특정 값을 수치로 입력하게 되고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746124-79e0aa00-aff0-11ea-8e40-37004805dc4d.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]19\" alt=\"RubberDuck\"></img><br/>\n",
        "에지전트는 입력된 값 만큼 행동하게 되죠.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWwA_GYmFXC0",
        "colab_type": "text"
      },
      "source": [
        "다음 **관측**이란 요소도 존재하는데 이 <u>관측은 환경에서 제공해주는 정보</u>입니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746321-bdd3af00-aff0-11ea-8a43-47f285edee6f.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]20\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이러한 관측은 **시각적 관측**과 **수치적 관측**으로 구분되는데,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746467-f3789800-aff0-11ea-9a6d-69c1f326de10.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]21\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "시각적 관측은 현재 상태의 정보를 이미지로 표현한 것이며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746573-21f67300-aff1-11ea-83cf-ef3cbffd5fa0.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]22\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "수치적 관측은 이미지의 형태가 아닌, 수치로만 표현한 것을 의미하죠.   \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746633-3cc8e780-aff1-11ea-87f4-a8d072a9b22f.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]23\" alt=\"RubberDuck\"></img><br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbEB5oz8GkZb",
        "colab_type": "text"
      },
      "source": [
        "그리고 가장 중요한 **보상함수**를 살펴보겠습니다.  \n",
        "\n",
        "보상함수는 에이전트가 특정 상태에서 특정 행동을 했을때  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746850-7d286580-aff1-11ea-91be-b82416058c12.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]24\" alt=\"RubberDuck\"></img><br/>\n",
        "보상을 받게 되고 에이전트는 이 보상정보를 통해 학습을 진행하게 되죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84746978-a5b05f80-aff1-11ea-89a4-7eaf1a7e83f9.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]25\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "일반적으로 현재 상태에서 특정 행동을 했을때 얻는 보상의 기대값을 이렇게 표기해요.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84747103-d2647700-aff1-11ea-9194-04baca9870ec.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]26\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이를 수식으로 표현하면 이렇게 표현할 수 있죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84747213-f2943600-aff1-11ea-9f22-b8b59a11d74f.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]27\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "하나씩 풀어 설명을 해보자면  \n",
        "현재상태에서 현재상태의 행동을 취해서  \n",
        "얻을 수 있는 **보상의 기대값**을 의미합니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84747466-3edf7600-aff2-11ea-8512-5a84603dc43a.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]28\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "우선 에이전트가 취한 행동에 따라 100% 확률로 선택한 방향으로 이동한다고 가정.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84747769-a85f8480-aff2-11ea-9cac-799efe492350.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]29\" alt=\"RubberDuck\"></img><br/>\n",
        "이때 왼쪽 행동을 선택하면 다음 상태는 현재 상태의 왼쪽 칸이 되겠죠.   \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84747931-eb215c80-aff2-11ea-9a1b-f9e10bfa0ae4.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]30\" alt=\"RubberDuck\"></img><br/>\n",
        "이를 함수의 형태로 표현하면 이렇게 표현할 수 있게 되고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84748067-173cdd80-aff3-11ea-9873-9e155db8f430.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]31\" alt=\"RubberDuck\"></img><br/>\n",
        "이와 같은 함수를 **보상함수**라 합니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1iy1MMLJrPe",
        "colab_type": "text"
      },
      "source": [
        "강화학습은 마치 자전거를 배우기 위해 여러번 넘어지듯 시행착오라는  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84748235-4fdcb700-aff3-11ea-892e-33577e1cf7c7.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]32\" alt=\"RubberDuck\"></img><br/>\n",
        "과정을 겪으며 보상을 최대화하는 의사결정 전략을 학습하는 것이라 이야기 했죠.  \n",
        "\n",
        "그럼 이러한 의사결정은 어떻게 학습을 할 수 있는 것 일까요??  \n",
        "\n",
        "강화학습에서는 에피소드가 끝나게 됐을때,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84748473-ad710380-aff3-11ea-9744-3aa73a772001.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]33\" alt=\"RubberDuck\"></img><br/>\n",
        "에이전트가 지나왔던 상태에서 했던 행동에 대한 정보를 기록하게 됩니다.  \n",
        "\n",
        "그리고 그 정보를 이영하여 그 다음 에피소드에 대한 의사결정을 하게 되죠.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84748697-f1640880-aff3-11ea-9379-c624c853e452.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]34\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 또 에피소드가 끝나면 이 에피소드를 통해 얻게 된 정보로 기록을  \n",
        "업데이트하며 이러한 과정을 반복하게 됩니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFI0ud7TLBA_",
        "colab_type": "text"
      },
      "source": [
        "그렇다면 어떠한 정보를 기록하게 됐을 때 좋은 의사결정을 내릴 수 있을 까요?  \n",
        "\n",
        "에이전트는 더 낳은 의사결정을 하기 위해 현재스텝에서 받았던 보상으로부터  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84748947-4bfd6480-aff4-11ea-8def-e1587f0e6a07.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]35\" alt=\"RubberDuck\"></img><br/>\n",
        "에피소드가 끝날 때까지 받았던 보상들을 더한 것을 정보로 이용하게 됩니다.  \n",
        "\n",
        "그림을 통해 학습과정을 살펴보면,  \n",
        "현재 모든 기록 정보는 0으로 초기화된 상황이라고 해봅시다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84749172-941c8700-aff4-11ea-91a0-fde6d0b86684.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]36\" alt=\"RubberDuck\"></img><br/>\n",
        "자 이렇게 빨간색 경로를 지나는 하나의 에피소드를 마치고 나서  \n",
        "현재 스텝으로부터 얻은 보상의 합 정보를 기록합니다.    \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84749431-ee1d4c80-aff4-11ea-9612-80cba0c28b54.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]37\" alt=\"RubberDuck\"></img><br/>\n",
        "그럼 빨간색 경로를 지나며 취한 행동에 대해서는 1, 그 이외는 0이 기록되게 되죠.  \n",
        "\n",
        "그렇다면 다음 에피소드를 실행할 때 이 정보를 바탕으로 행동한다면 어떻게 될까요?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84749751-59ffb500-aff5-11ea-9ba3-4c0d70021b61.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]38\" alt=\"RubberDuck\"></img><br/>\n",
        "에이전트는 자연스럽게 자신의 상태에서 기록한 값이 놓은 행동만 하게 되며  \n",
        "목적지에 도착할 수 있게 됩니다.  \n",
        "하지만 현재 이 루트에는 조금 불필요한 경로가포함되어 있네요.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwMDoBDBOIbY",
        "colab_type": "text"
      },
      "source": [
        "이때 이 파란색 루트가 새롭게 추가되었을 때 과연 둘 중 어느 경로가 더 효율적일까요?  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84750087-cf6b8580-aff5-11ea-8df2-718ecfd1050d.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]39\" alt=\"RubberDuck\"></img><br/>\n",
        "당연히 이 파란색 루트가 더 효율적이겠죠?  \n",
        "\n",
        "하지만 에이전트는 초기상태에서 왼쪽으로 갈지, 위로 갈지 판단할 수 없어요.  \n",
        "그 이유는 당연히 현재 기록한 정보만으로는 이 길이 효율적인지 알수 없기 때문이죠.  \n",
        "\n",
        "그래서 이 점을 보완하기 위해 **감가율**이라는 개념이 도입됩니다. \n",
        "감가율은 통상 그리스 문자중 세번째 감마를 활용하여 표기하며,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84750520-54ef3580-aff6-11ea-8fd5-1f2948ace441.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]40\" alt=\"RubberDuck\"></img><br/>\n",
        "0부터 1사이의 값으로 설정, 그리고 1에 가까울수록  \n",
        "미래의 보상에 더 많은 가중치를 두게 돼요.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84750731-a39ccf80-aff6-11ea-9724-bc64e1475d7e.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]41\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "이제 감가율이 반영된 보상정보를 기록해 보면 현재 스텝으로 부터 받았던 보상부터\n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84751328-6553e000-aff7-11ea-839e-7ad87a62573f.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]42\" alt=\"RubberDuck\"></img><br/>\n",
        "에피소드가 끝날때까지 받았던 보상들에 감가율을 스텝 차이만큼 곱해서 더해주게 됩니다.  \n",
        "\n",
        "그리고 이 값을 반환값이라고 부르게 되죠.  \n",
        "현재 스텝에서의 반환값은 일반적으로 대문자G라고 표기하며,  \n",
        "반환값을 수식으로 표현하자면 이러한 수식으로 표현할 수 있습니다.  \n",
        "수식으로만 접하게 되면 조금 난해할 수 있는데,  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV2jjmUqRSVr",
        "colab_type": "text"
      },
      "source": [
        "이걸 간단하게 그림으로 그려본다면 어떻게 되는지 살펴보도록 합시다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84751626-be237880-aff7-11ea-85bd-e72ffb367280.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]43\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "반환값을 기록할때는 종료된 상태부터 처음 상태까지 거꾸로 계산하는게 좀더 쉬우니,  \n",
        "역으로 계산을 진행해보도록 하겠습니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84751958-25d9c380-aff8-11ea-8437-a15bbdf0857c.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]44\" alt=\"RubberDuck\"></img><br/>\n",
        "환경에서의 보상은 목적지에 도착할 때에만 얻기 때문에 \n",
        "각 스텝의 반환값은 이렇게 되며, 여기 대문자T는 에피소드가 종료된 스텝을 의미해요.  \n",
        "\n",
        "그리고 이렇게 반환값을 기록하게 되면 에이전트는 이제 어느 경로가 효율적인지 판단할 수 있게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84752273-823ce300-aff8-11ea-964b-ff1f0db83973.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]45\" alt=\"RubberDuck\"></img><br/>\n",
        "\n",
        "그런데 이때, 생각을 좀 더 해보면, 에이전트는 처음에 이러한 길을 찾아 탐색을 진행 했고,  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84752561-dc3da880-aff8-11ea-9775-66b8ea48d117.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]46\" alt=\"RubberDuck\"></img><br/>\n",
        "이제 이 길은 에이전트가 모르는 길보다는 훨씬 더 괜찮다 판단할 수 있지 않을까요?  \n",
        "\n",
        "사람도 마찬가지로 새롭게 이사한 집을 가기 위해 찾아놓은 길로만 다니는 것처럼 말이죠.  \n",
        "\n",
        "그래서 가끔은 에이전트에게 무작위로 움직이게 설정하여  \n",
        "여러 경로를 시도해 보라는 **'탐험'**이라는 개념이 추가되게 됩니다.  \n",
        "<img src=\"https://user-images.githubusercontent.com/54702627/84752909-553d0000-aff9-11ea-8fd8-3283c546a95f.png\" width=\"450px\" height=\"300px\" title=\"강화학습[1]47\" alt=\"RubberDuck\"></img><br/>\n",
        "그리고 이 탐험과 대립되는 개념은 **'이용'**이라는 개념으로,  \n",
        "에이전트가 찾아놓은 길로 하여 계속해서 선택하고 움직이게 되는 것을 의미하죠.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upR-xRsUBcEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}